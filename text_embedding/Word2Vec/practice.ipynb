{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### 영어 Word2Vec만들기(네이버 영화 리뷰)"
      ],
      "metadata": {
        "id": "jQ0M3iRdN5_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy\n",
        "!pip install mecab-python\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4wQoNuU0C7d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_pS7XJACr_z"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "uK2RAP-UDvUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
      ],
      "metadata": {
        "id": "AkRr-SqzCtp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 데이터인 <content></content> 사이의 내용만 추출\n",
        "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "sent_text = sent_tokenize(content_text)\n",
        "\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "    normalized_text.append(tokens)\n",
        "\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]"
      ],
      "metadata": {
        "id": "KH0pdvE9CvVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('총 샘플의 수: ', len(result))"
      ],
      "metadata": {
        "id": "RSd1PTC8Cww7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result[0])"
      ],
      "metadata": {
        "id": "hhX2ScjfGSoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "ii4uqaNsGeoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "9F-2Y_yLGXev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec 하이퍼파라미터\n",
        "# vector_size: 임베딩 된 벡터의 차원\n",
        "# window: 컨텍스트 윈도우 크기\n",
        "# min_count: 단어 최소 빈도 수 제한(min_count 아래의 단어는 학습하지 않는다.)\n",
        "# workers: 학습을 위한 프로세스 수\n",
        "# sg: CBOW=0, SKIP-gram=1\n",
        "model = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
      ],
      "metadata": {
        "id": "LvAzLxTuGme_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# man과 가장 유사한 단어 찾기(wv.most_similar())\n",
        "model_result = model.wv.most_similar(\"man\")\n",
        "print(model_result)"
      ],
      "metadata": {
        "id": "ldcwjJx_GuyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장하고 로드하기\n",
        "model.wv.save_word2vec_format('eng_w2v')\n",
        "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\")"
      ],
      "metadata": {
        "id": "x3fJnYp8LlV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "7pZpCMlRj6ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 한국어 Word2Vec 만들기(네이버 영화 리뷰)"
      ],
      "metadata": {
        "id": "zIMTLn-WN-eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "KerCQMaDLyIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
      ],
      "metadata": {
        "id": "cr2O2cWnOKis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_table('ratings.txt')"
      ],
      "metadata": {
        "id": "KjFTRm6NORom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "p1BaMCthOVMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.isnull().values.any()"
      ],
      "metadata": {
        "id": "nUEDSj72OXQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.dropna(how=\"any\")\n",
        "len(train_data)"
      ],
      "metadata": {
        "id": "2-IsidIgUHAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한글이 아닌 텍스트 제거\n",
        "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣]\", \"\", regex=True)"
      ],
      "metadata": {
        "id": "wbJa0ETwUmIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
      ],
      "metadata": {
        "id": "rBPVkDAIUzOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "train_data = train_data[:50000]\n",
        "\n",
        "tokenized_data = []\n",
        "for sentence in tqdm.tqdm(train_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True)\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords]\n",
        "    tokenized_data.append(stopwords_removed_sentence)"
      ],
      "metadata": {
        "id": "u_vht7rzU9Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist([len(review) for review in tokenized_data], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vkgmPfhSVZW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(sentences = tokenized_data, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
      ],
      "metadata": {
        "id": "rhmPjr7wxaIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.vectors.shape"
      ],
      "metadata": {
        "id": "ZawNzmNDx6Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.wv.most_similar(\"최민식\"))"
      ],
      "metadata": {
        "id": "e-26vjLMx93F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.wv.most_similar(\"사랑\"))"
      ],
      "metadata": {
        "id": "WD80pSmHzGYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output env_w2v"
      ],
      "metadata": {
        "id": "sibEc7E8zJZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IfGgFZFqzd1F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}